% !TEX root = ../main.tex

\chapter{Fundamentação teórica}
\label{cap:fundamentacao}

\section{Inteligência Artificial e o problema proposto} \label{sec:ia-e-ml}

Inteligência Artificial (IA) é o campo que estuda agentes capazes de perceber o ambiente e executar ações orientadas a metas; dentro dele, o Aprendizado de Máquina (ML) trata de métodos que permitem a esses agentes melhorar o desempenho a partir de dados e experiência \cite{russell2021artificial}. Em termos práticos, em vez de codificar regras fixas, ML aprende funções de predição a partir de exemplos históricos, sendo especialmente útil quando não há um modelo analítico único que explique adequadamente o fenômeno \cite{andrianarivony2024review}.

A previsão de focos de incêndio no Cerrado a partir de variáveis climáticas se encaixa diretamente nesse enquadramento. Trata-se de um sistema multivariado, com relações não lineares e dependências espaço-temporais, para o qual existem séries históricas abundantes, mas não um conjunto de equações fechado que, sozinho, capte todas as interações relevantes. Assim, a abordagem orientada a dados é adequada: aprende-se, a partir de observações passadas, uma função que mapeia condições meteorológicas e contextuais para risco de ocorrência.

Além disso, há evidência empírica de que o fogo no Cerrado não é aleatório. Sua distribuição apresenta sazonalidade marcada, com concentração na estação seca, dependência do tipo de cobertura e associação com vetores de desmatamento, indicando padrões reprodutíveis que podem ser aprendidos por modelos \cite{nascimento2011analise}.

A literatura sustenta a resolução do problema com aprendizado de máquina para classificação probabilística, em que estima-se $p(\text{FOCO}=1 \mid \mathbf{x})$ e define-se o limiar de decisão conforme os custos operacionais. Pretende-se, portanto, a comparação de modelos supervisionados clássicos e ensembles de árvores, priorizando saídas calibráveis e interpretação alinhada ao domínio.

\section{Trabalhos relacionados}
\label{sec:trabalhos_relacionados}

A literatura recente aponta um crescimento expressivo do uso de técnicas de aprendizado de máquina e inteligência artificial na previsão de ocorrência e comportamento de incêndios florestais. Os trabalhos variam quanto ao tipo de tarefa (ocorrência versus severidade ou área queimada), à resolução espacial e temporal e ao conjunto de variáveis preditoras utilizadas, como meteorologia, topografia, combustível, fatores antrópicos e índices espectrais \cite{andrianarivony2024review}.

Do ponto de vista ecológico e climatológico, estudos clássicos sobre o bioma Cerrado mostram que a distribuição espacial e temporal dos focos de calor não é aleatória. Existe uma forte sazonalidade associada ao regime de chuvas e à fenologia da vegetação, bem como influência de gradientes de uso do solo e de desmatamento \cite{nascimento2011analise}. Essa evidência de estruturas determinísticas e multimodais no padrão de fogo fundamenta o uso de modelos de aprendizado de máquina, que são capazes de explorar relações não lineares entre múltiplas variáveis explicativas.

No contexto brasileiro, Silva e White \cite{silva2016deteccao} utilizaram a série de focos do Programa Queimadas entre 1999 e 2015 para quantificar a incidência de fogo nos diferentes biomas. O estudo trabalha com agregações anuais e mensais e identifica diferenças marcantes na sazonalidade, com destaque para o Cerrado e o Pantanal. Jesus et al. \cite{jesus2020analise} avançaram nessa análise ao investigar a incidência temporal e espacial em biomas e unidades de conservação entre 2003 e 2017. A metodologia combinou estimativa de densidade Kernel para identificação de hotspots com testes de tendência de Mann-Kendall, indicando concentrações persistentes em transições de biomas, como a região Amazônia-Cerrado.

Já Alves et al. \cite{alves2021estudo} integraram focos de calor, reanálises climáticas e índices de grande escala para investigar a relação entre variáveis meteorológicas e a atividade de fogo. Embora esses trabalhos forneçam uma base sólida de caracterização do regime de fogo, eles se concentram em estatística descritiva ou correlações lineares, sem o emprego de algoritmos de aprendizado de máquina supervisionados para predição.

A aplicação direta de ML para estimativa de suscetibilidade foi explorada por Freitas et al. \cite{freitas2025xingu} na Amazônia. O trabalho utilizou modelos Random Forest e XGBoost calibrados com variáveis topográficas, climáticas e de uso da terra, resultando em mapas de suscetibilidade com desempenho elevado. No âmbito algorítmico, Shu et al. \cite{shu2021towards} desenvolveram o método Double Weighted Naive Bayes with Compensation Coefficient (DWCNB) para predição de fogo. O diferencial dessa abordagem é a atribuição de pesos tanto aos atributos quanto aos seus valores, atenuando a suposição de independência condicional. O modelo utiliza um coeficiente de compensação ajustado por testes ortogonais para equilibrar as probabilidades a priori, alcançando ganhos de acurácia significativos em relação ao Naive Bayes convencional.

Globalmente, revisões como a de Andrianarivony e Akhloufi \cite{andrianarivony2024review} indicam que modelos baseados em árvores de decisão e ensembles (Random Forest, Gradient Boosting e XGBoost) são os mais recorrentes para tarefas de risco de fogo devido à capacidade de lidar com variáveis heterogêneas. Métodos de boosting constroem sequências de modelos fracos de maneira iterativa para corrigir erros anteriores \cite{friedman2001greedy}. Outros estudos em regiões mediterrâneas e na China reforçam a importância de integrar variáveis climáticas com técnicas de explicabilidade (XAI) e estratégias de validação que respeitem a estrutura espacial e temporal dos dados \cite{cilli2022xai, pang2022china, ahajjam2025wildfire}.

\section{Modelos de classificação supervisionada}

No contexto deste trabalho, o problema de previsão de ocorrência de fogo é formulado
como uma tarefa de classificação supervisionada binária: para cada instância representada por
um vetor de atributos $\mathbf{x} \in \mathbb{R}^p$, associado a condições ambientais e antrópicas,
busca-se estimar a probabilidade de ocorrência de um evento de interesse $y \in \{0,1\}$, como,
por exemplo, a presença de focos de calor em uma dada região e período. Em termos gerais, um
modelo de aprendizado de máquina supervisionado tenta aproximar uma função
$f_\theta : \mathbb{R}^p \rightarrow [0,1]$ parametrizada por $\theta$, de modo que $f_\theta(\mathbf{x})$
represente uma estimativa de $P(Y=1 \mid \mathbf{x})$ \cite{mitchell1997machine,russell2016artificial}.

Diversos algoritmos podem ser empregados para construir essa aproximação a partir de
um conjunto de treinamento rotulado. Neste trabalho são considerados modelos clássicos de
classificação largamente utilizados em aplicações reais: Regressão Logística, Naive Bayes,
Máquinas de Vetores de Suporte (SVM), Random Forest e XGBoost. Esta seção apresenta os
fundamentos teóricos de cada um desses métodos, com ênfase em seus princípios de funcionamento,
hipóteses subjacentes e implicações para tarefas de previsão em dados ambientais.


\subsection{Regressão Logística}

A Regressão Logística é um modelo discriminativo que estima diretamente a probabilidade
condicional de uma classe binária a partir de uma combinação linear dos preditores. Seja
$\mathbf{x} = (x_1,\ldots,x_p)$ o vetor de atributos e $y \in \{0,1\}$ a variável resposta. O modelo
assume que o logit da probabilidade de $Y=1$ é uma função linear de $\mathbf{x}$:

\begin{equation}
\label{eq:logistic_model}
\text{logit}\bigl(P(Y=1 \mid \mathbf{x})\bigr)
= \log \left( \frac{P(Y=1 \mid \mathbf{x})}{1 - P(Y=1 \mid \mathbf{x})} \right)
= \beta_0 + \sum_{j=1}^p \beta_j x_j.
\end{equation}

Aplicando-se a função logística, obtém-se a probabilidade modelada:

\begin{equation}
P(Y=1 \mid \mathbf{x}) = \sigma(\beta_0 + \mathbf{x}^\top \boldsymbol{\beta}) 
= \frac{1}{1 + e^{-(\beta_0 + \mathbf{x}^\top \boldsymbol{\beta})}}.
\end{equation}

Os parâmetros $\beta_0, \boldsymbol{\beta}$ são estimados geralmente por máxima verossimilhança,
equivalente à minimização da perda logarítmica (entropia cruzada). Para um conjunto de dados
$\{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^n$, a função de custo $\mathcal{L}$ é dada por:

\begin{equation}
\mathcal{L}(\beta_0, \boldsymbol{\beta})
= - \sum_{i=1}^n \Bigl[ y^{(i)} \log \hat{p}^{(i)} 
+ (1 - y^{(i)}) \log(1 - \hat{p}^{(i)}) \Bigr],
\end{equation}

em que $\hat{p}^{(i)} = P(Y=1 \mid \mathbf{x}^{(i)})$.
A minimização é usualmente realizada por métodos numéricos iterativos, como gradiente
descendente, Newton-Raphson ou variantes quasi-Newton.

A Regressão Logística apresenta duas vantagens importantes em aplicações científicas:
(i) fornece probabilidades calibradas de ocorrência do evento, o que permite avaliar incertezas
e definir limiares de decisão dependentes de custo; e (ii) permite interpretação direta dos
coeficientes em termos de razão de chances (odds ratio): $e^{\beta_j}$ representa o fator de
multiplicação na razão de chances associado a um incremento unitário em $x_j$, mantendo-se
os demais atributos constantes. Em problemas ambientais, essa interpretabilidade é útil para
quantificar o efeito relativo de variáveis climáticas ou de uso do solo sobre a probabilidade de
fogo.

Para evitar sobreajuste, é comum empregar regularização L2 (ridge) ou L1 (lasso), que
introduzem penalizações sobre a magnitude dos coeficientes, favorecendo modelos mais simples
e, no caso da regularização L1, promovendo seleção automática de variáveis.


\subsection{Classificadores Naive Bayes}

Modelos Naive Bayes são classificadores probabilísticos baseados no Teorema de Bayes.
Dado um conjunto de atributos $\mathbf{x}$ e uma variável de classe $Y$, tem-se:

\begin{equation}
P(Y = c \mid \mathbf{x}) = \frac{P(\mathbf{x} \mid Y=c) P(Y=c)}{P(\mathbf{x})},
\end{equation}

em que $P(\mathbf{x})$ é o mesmo para todas as classes e pode ser tratado como constante
no processo de classificação. A ideia central é modelar a distribuição dos atributos condicionada
à classe, $P(\mathbf{x} \mid Y=c)$, e a distribuição a priori $P(Y=c)$.

O adjetivo \emph{naive} decorre da hipótese de independência condicional entre os atributos
dado o valor da classe:

\begin{equation}
P(\mathbf{x} \mid Y=c) = \prod_{j=1}^p P(x_j \mid Y=c).
\end{equation}

Essa suposição é frequentemente irrealista em dados reais, especialmente em domínios
ambientais em que variáveis meteorológicas e de vegetação são fortemente correlacionadas.
Ainda assim, o modelo apresenta bom desempenho em diversas aplicações, sobretudo quando
o número de atributos é grande em relação ao tamanho da amostra e quando as dependências
entre variáveis não são extremamente críticas para a discriminação entre as classes.

Na prática, diferentes variantes são utilizadas conforme a natureza dos atributos:
Naive Bayes gaussiano (atributos contínuos modelados como gaussianos condicionados à
classe), multinomial (contagens) ou bernoulli (atributos binários). A classificação consiste em
escolher a classe que maximiza a probabilidade a posteriori $P(Y=c \mid \mathbf{x})$. Devido à
sua formulação fechada, o treinamento é computacionalmente muito eficiente, o que o torna um
bom modelo de referência (baseline) em experimentos comparativos.


\subsection{Máquinas de Vetores de Suporte (SVM)}

Máquinas de Vetores de Suporte são modelos discriminativos que buscam encontrar um
hiperplano de separação entre classes com margem máxima \cite{cortes1995svm}. No caso
linearmente separável, considera-se um hiperplano definido por $(\mathbf{w}, b)$ tal que:

\begin{equation}
\mathbf{w}^\top \mathbf{x} + b = 0,
\end{equation}

e impõe-se que, para cada amostra rotulada $(\mathbf{x}^{(i)}, y^{(i)})$, com
$y^{(i)} \in \{-1, +1\}$:

\begin{equation}
y^{(i)} \bigl(\mathbf{w}^\top \mathbf{x}^{(i)} + b\bigr) \geq 1.
\end{equation}

A SVM procura o hiperplano que maximiza a margem, isto é, a distância entre as fronteiras
de decisão e os pontos mais próximos (vetores de suporte). Isso equivale a resolver o problema
de otimização:

\begin{equation}
\min_{\mathbf{w}, b} \frac{1}{2} \lVert \mathbf{w} \rVert^2
\quad \text{sujeito a} \quad
y^{(i)} \bigl(\mathbf{w}^\top \mathbf{x}^{(i)} + b\bigr) \geq 1,\ \forall i.
\end{equation}

Em situações não separáveis, introduzem-se variáveis de folga $\xi_i$ e um parâmetro de
penalização $C>0$ que controla o equilíbrio entre largura da margem e erros de classificação.
A formulação dual do problema permite incorporar funções de kernel $K(\mathbf{x},\mathbf{x}')$,
o que viabiliza fronteiras de decisão não lineares sem necessidade de explicitar a transformação
para espaços de alta dimensão (``truque do kernel'').

SVMs são particularmente úteis quando há fronteira de decisão complexa em um espaço
de atributos moderadamente dimensionado e quando se deseja maximizar a margem entre
classes, o que tende a melhorar a capacidade de generalização. Em contrapartida, o custo
computacional pode se tornar elevado em bases muito grandes, e a interpretação dos modelos é
menos direta quando kernels não lineares são adotados.


\subsection{Árvores de decisão e Random Forest}

Árvores de decisão são modelos fundamentados em partições recursivas do espaço de atributos. O processo consiste em dividir o conjunto de dados em subconjuntos cada vez mais homogêneos em relação à variável alvo $y$. Para um nó $m$, representando um subconjunto dos dados $Q_m$ com $n_m$ observações, a proporção de instâncias da classe $k$ é definida como:

\begin{equation}
p_{mk} = \frac{1}{n_m} \sum_{\mathbf{x}^{(i)} \in Q_m} I(y^{(i)} = k),
\end{equation}

em que $I(\cdot)$ é a função indicadora. A escolha do melhor ponto de corte em cada nó baseia-se em medidas de impureza. A métrica mais utilizada em algoritmos de classificação é o índice de Gini ($G$), que quantifica a probabilidade de uma instância escolhida aleatoriamente ser rotulada incorretamente, sendo expresso por:

\begin{equation}
G(m) = \sum_{k=1}^K p_{mk} (1 - p_{mk}) = 1 - \sum_{k=1}^K p_{mk}^2.
\end{equation}

O algoritmo busca, para cada nó, a variável $j$ e o limiar $s$ que minimizam a impureza combinada dos nós filhos (esquerdo e direito), resolvendo o seguinte problema de otimização:

\begin{equation}
J(j, s) = \frac{n_{left}}{n_m} G(left) + \frac{n_{right}}{n_m} G(right).
\end{equation}



Embora árvores individuais sejam modelos interpretáveis, elas tendem a apresentar alta variância, pois pequenas variações nos dados de treinamento podem resultar em estruturas de decisão drasticamente diferentes. O Random Forest \cite{breiman2001randomForest} atenua essa limitação por meio de um método de comitê (\textit{ensemble}) baseado em \textit{bagging} (bootstrap aggregating). O algoritmo constrói $B$ árvores independentes, cada uma treinada em uma amostra bootstrap $Z^*$ obtida com reposição do conjunto original.

O diferencial do Random Forest em relação ao \textit{bagging} convencional é a introdução do método de subespaços aleatórios: em cada divisão de cada árvore, o algoritmo seleciona apenas um subconjunto aleatório de $m \approx \sqrt{p}$ atributos para considerar como candidatos ao corte. Essa técnica reduz a correlação entre as árvores da floresta, garantindo que o erro médio do comitê seja menor que o erro médio de suas árvores individuais.

A predição final para uma nova instância $\mathbf{x}$ é obtida por votação majoritária no caso de classificação:

\begin{equation}
\hat{y} = \text{moda} \{ \hat{C}_1(\mathbf{x}), \hat{C}_2(\mathbf{x}), \dots, \hat{C}_B(\mathbf{x}) \},
\end{equation}

em que $\hat{C}_b(\mathbf{x})$ é a classe prevista pela $b$-ésima árvore. Além da robustez preditiva, o modelo permite estimar a importância de cada atributo via \textit{Mean Decrease Gini}, que calcula a redução total na impureza dos nós trazida por uma variável específica ao longo de todas as árvores da floresta, fornecendo interpretação relevante para o domínio do problema \cite{russell2016artificial}.


\subsection{Gradient Boosting e XGBoost}

Ao passo que o bagging reduz a variância ao combinar modelos treinados de forma
independente, métodos de boosting constroem sequências de modelos fracos de maneira
iterativa, cada novo modelo focando em corrigir os erros cometidos pelos anteriores
\cite{friedman2001greedy}. No Gradient Boosting, em particular, formula-se a tarefa de
aprendizado como um problema de minimização de uma função de perda diferenciável
$\mathcal{L}(y, F(\mathbf{x}))$. Parte-se de um modelo inicial $F_0$ e, a cada iteração $m$, ajusta-se
um novo modelo fraco $h_m$ aos resíduos negativos do gradiente da perda em relação às
previsões correntes, atualizando:

\begin{equation}
F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \nu \cdot h_m(\mathbf{x}),
\end{equation}

em que $\nu \in (0,1]$ é a taxa de aprendizado. Quando os modelos fracos $h_m$ são árvores
de decisão de pequena profundidade, obtém-se um ensemble capaz de capturar interações
complexas entre atributos.

O XGBoost (\emph{Extreme Gradient Boosting}) \cite{chen2016xgboost} é uma implementação
otimizada de gradient boosting com árvores, amplamente utilizada em aplicações práticas pela
combinação de alto desempenho preditivo e eficiência computacional. Entre suas principais
características estão: (i) regularização explícita da complexidade das árvores na função objetivo,
controlando profundidade e número de folhas; (ii) uso de aproximações de segunda ordem
(gradiente e hessiano) para acelerar o processo de otimização; (iii) suporte nativo a paralelismo,
amostragem de instâncias e de atributos; e (iv) tratamento eficiente de valores ausentes.

Em tarefas de previsão de incêndios, XGBoost e métodos similares de gradient boosting
têm se mostrado particularmente competitivos, pois conseguem explorar relações não lineares
entre variáveis meteorológicas, de combustíveis e de uso do solo, ao mesmo tempo em que lidam
bem com atributos heterogêneos e correlações complexas \cite{andrianarivony2024review}.


\section{Avaliação de modelos de classificação}

A avaliação rigorosa de modelos de classificação é parte essencial da fundamentação
metodológica, mas alguns conceitos básicos são discutidos aqui por estarem intimamente
relacionados à interpretação dos resultados de modelos supervisionados. Em problemas
binários, um modelo produz, para cada instância, uma classe predita e, em muitos casos, uma
probabilidade estimada. Comparando-se as predições com os rótulos reais, obtém-se a matriz
de confusão, com contagens de verdadeiros positivos (VP), falsos positivos (FP), verdadeiros
negativos (VN) e falsos negativos (FN).

A partir dessas quantidades derivam-se métricas como acurácia, precisão, revocação
(sensibilidade), especificidade e $F_1$-score. Para problemas em que a classe positiva é rara,
como a ocorrência de fogo em resolução espacial e temporal fina, métricas baseadas apenas na
acurácia podem ser enganosas: um classificador trivial que prevê sempre a classe negativa pode
apresentar alta acurácia, mas utilidade prática nula. Nesses cenários, medidas sensíveis ao
desbalanceamento, como a área sob a curva ROC (AUC) ou a área sob a curva de precisão-revocação,
são mais informativas \cite{fawcett2006roc}.

Outro aspecto fundamental é o esquema de validação. Técnicas como validação cruzada
$k$-fold ou separação explícita de conjuntos de treino e teste, respeitando dependências
espaciais e temporais, são necessárias para obter estimativas não enviesadas de desempenho.
No caso de séries ambientais, é importante evitar vazamento de informação (por exemplo,
amostras muito próximas no tempo ou no espaço aparecendo simultaneamente em treino e
teste), pois isso tende a inflar artificialmente as métricas de avaliação
\cite{nascimento2011analise,andrianarivony2024review}.
