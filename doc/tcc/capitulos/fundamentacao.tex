% !TEX root = ../main.tex

\chapter{Fundamentação teórica}
\label{cap:fundamentacao}

Este capítulo apresenta a base conceitual que sustenta as escolhas metodológicas deste trabalho. A fundamentação está organizada em três eixos principais: (i) inteligência artificial e aprendizado de máquina aplicados à previsão de queimadas, (ii) extração e organização de dados em pipelines climáticos e de focos e (iii) tratamento de dados faltantes em séries temporais ambientais, com ênfase em variáveis meteorológicas do INMET e na construção de diferentes cenários de base de modelagem.

\section{Inteligência Artificial e o problema proposto} \label{sec:ia-e-ml}

Inteligência Artificial (IA) é o campo que estuda agentes capazes de perceber o ambiente e executar ações orientadas a metas; dentro dele, o Aprendizado de Máquina (ML) trata de métodos que permitem a esses agentes melhorar o desempenho a partir de dados e experiência \cite{russell2021artificial}. Em termos práticos, em vez de codificar regras fixas, ML aprende funções de predição a partir de exemplos históricos, sendo especialmente útil quando não há um modelo analítico único que explique adequadamente o fenômeno \cite{andrianarivony2024review}.

A previsão de focos de incêndio no Cerrado a partir de variáveis climáticas se encaixa diretamente nesse enquadramento. Trata-se de um sistema multivariado, com relações não lineares e dependências espaço-temporais, para o qual existem séries históricas abundantes, mas não um conjunto de equações fechado que, sozinho, capte todas as interações relevantes. Assim, a abordagem orientada a dados é adequada: aprende-se, a partir de observações passadas, uma função que mapeia condições meteorológicas e contextuais para risco de ocorrência.

Além disso, há evidência empírica de que o fogo no Cerrado não é aleatório. Sua distribuição apresenta sazonalidade marcada, com concentração na estação seca, dependência do tipo de cobertura e associação com vetores de desmatamento, indicando padrões reprodutíveis que podem ser aprendidos por modelos \cite{nascimento2011analise}.

A literatura sustenta a resolução do problema com aprendizado de máquina para classificação probabilística, em que estima-se $p(\text{FOCO}=1 \mid \mathbf{x})$ e define-se o limiar de decisão conforme os custos operacionais. Pretende-se, portanto, a comparação de modelos supervisionados clássicos e ensembles de árvores, priorizando saídas calibráveis e interpretação alinhada ao domínio.
A regressão logística é um modelo discriminativo que estima a probabilidade pela função sigmoide
$p(y{=}1 \mid \mathbf{x}) = \sigma(\mathbf{w}^\top \mathbf{x} + b)$, com $\sigma(z)=\tfrac{1}{1+e^{-z}}$.
Os parâmetros são obtidos pela minimização da perda logística regularizada,
$\min_{\mathbf{w},b}\ \sum_{i} \log\bigl(1+\exp(-y_i(\mathbf{w}^\top \mathbf{x}_i+b))\bigr) + \lambda \lVert \mathbf{w}\rVert_2^2$,
o que fornece escores diretamente interpretáveis como risco estimado \cite{mitchell1997machine,russell2021artificial}.

O Naive Bayes é um modelo gerativo que supõe independência condicional dos atributos dado a classe: $p(y \mid \mathbf{x}) \propto p(y)\prod_j p(x_j \mid y)$. Na variante Gaussiana, $x_j \mid y \sim \mathcal{N}(\mu_{jy},\sigma_{jy}^2)$, e a decisão decorre da comparação do log quociente de verossimilhanças somado ao log das prioris. Apesar da hipótese forte de independência, é um baseline robusto e computacionalmente leve \cite{mitchell1997machine,russell2021artificial}.

A SVM linear é um classificador de margem máxima que resolve o problema de otimização convexa
$\min_{\mathbf{w},b,\boldsymbol{\xi}}\ \tfrac{1}{2}\lVert \mathbf{w}\rVert^2 + C\sum_i \xi_i\ \ \text{sujeito a}\ \ y_i(\mathbf{w}^\top \mathbf{x}_i+b) \ge 1-\xi_i,\ \xi_i \ge 0$.
O modelo produz escores $s(\mathbf{x})=\mathbf{w}^\top \mathbf{x}+b$ que podem ser calibrados para probabilidade a posteriori \cite{russell2021artificial}.

As árvores de decisão particionam o espaço por testes axis-aligned escolhidos para maximizar a redução de impureza. Com índice de Gini $G=1-\sum_c p_c^2$ ou entropia $H=-\sum_c p_c \log p_c$, o ganho no nó é $\Delta I = I(\text{pai}) - \sum_k \tfrac{n_k}{n} I(\text{filho}_k)$. A probabilidade na folha é a frequência empírica da classe positiva \cite{russell2021artificial}. O Random Forest agrega muitas árvores treinadas sobre amostras bootstrap e subconjuntos aleatórios de atributos, reduzindo variância e capturando interações não lineares; a predição é a média das probabilidades individuais \cite{breiman2001randomForest}. O ExtraTrees intensifica a aleatoriedade ao amostrar pontos de corte antes de avaliar o ganho, o que tende a diminuir correlação entre árvores e melhorar a generalização por agregação.

Os métodos de boosting constroem um modelo aditivo $F_T(\mathbf{x})=\sum_{t=1}^{T}\eta\, h_t(\mathbf{x})$ em que cada árvore rasa $h_t$ é ajustada ao gradiente negativo da perda avaliada em $F_{t-1}$. O XGBoost estende essa ideia com uma aproximação de segunda ordem da perda e regularização dos pesos das folhas,
$\mathcal{L} \approx \sum_i \bigl(g_i f(\mathbf{x}_i) + \tfrac{1}{2} h_i f(\mathbf{x}_i)^2\bigr) + \Omega(f)$, com $\Omega(f)=\gamma T + \tfrac{\lambda}{2}\sum_j w_j^2$,
além de subamostragem e otimizações de engenharia \cite{chen2016xgboost}. Variações modernas como LightGBM utilizam histogramas e crescimento leaf-wise sob restrições de profundidade, e o CatBoost emprega técnicas ordenadas para tratar atributos categóricos e reduzir vazamento estatístico; ambas mantêm a ideia central de boosting com árvores e produzem escores probabilísticos adequados à calibração.

\section{Por que restringir o escopo ao Cerrado?}
\label{sec:porque-cerrado}

\subsection{Limitação computacional e escopo de pesquisa}
Modelar, de forma unificada, a ocorrência de focos em todos os biomas brasileiros implica acomodar múltiplos regimes de fogo, conjuntos de combustíveis com propriedades distintas e respostas climáticas heterogêneas. Essa heterogeneidade amplia o espaço de hipóteses do modelo, eleva o custo de busca de hiperparâmetros e aumenta a chance de sobreajuste. Dado o poder computacional disponível e o objetivo de assegurar validade interna, adota-se um recorte por bioma que concentra a capacidade computacional na escolha e validação de variáveis dentro de um único regime de fogo, reduzindo não estacionariedade e favorecendo testes de generalização espacial consistentes.

\subsection{Heterogeneidade entre biomas e implicações para a consistência do modelo}
\label{subsec:heterogeneidade-biomas}

A literatura de ecologia do fogo demonstra que biomas brasileiros se distribuem em categorias com respostas intrinsecamente distintas ao fogo, como ecossistemas dependentes, sensíveis e independentes. Essa classificação emerge de diferenças de composição vegetal, clima e fontes de ignição que se traduzem em regimes contrastantes de frequência, sazonalidade, intensidade e severidade dos incêndios \cite{ramalho2024compreendendo}.

No confronto direto entre Cerrado e Amazônia, por exemplo, o primeiro apresenta estação seca longa, abundância de combustível fino herbáceo que seca rapidamente e ignições naturais por raios no início das chuvas, ao passo que a Amazônia é sensível ao fogo e, historicamente, quase não queima na ausência de secas anômalas e distúrbios antrópicos. As implicações de manejo também divergem, o que evidencia que as “regras” do processo gerador de dados mudam com o bioma \cite{pivello2011use}.

Em escala nacional, padrões climáticos relacionados ao fogo e métricas de risco revelam contrastes sistemáticos entre biomas: a persistência temporal do fogo é generalizada, mas a adequação térmica é elevada nos biomas dependentes e mais baixa nas florestas úmidas, implicando sensibilidades distintas às variáveis climáticas e, portanto, funções de resposta diferentes quando se modela a ocorrência \cite{viegas2022fireclimate}.

Mesmo dentro de um único bioma, como o Cerrado, há controles climáticos com estrutura sazonal e ecorregional específicos: pré-condições no outono austral (março a maio) modulam a disponibilidade e condição do combustível no início da estação de fogo, enquanto condições concorrentes durante agosto a outubro explicam a variação no final da estação. Esses efeitos variam por mês e por ecorregião, reforçando que agregar regimes distintos em um único modelo dilui o sinal e compromete a transferibilidade \cite{silva2025climatic}.


\section{Pipeline de dados} \label{sec:pipelines}

A previsão de focos de incêndio em escala regional requer integração consistente de fontes heterogêneas - focos detectados por satélite, séries meteorológicas e variáveis derivadas. No Cerrado, a sazonalidade e a estrutura espacial dos focos, associadas a cobertura vegetal e pressão antrópica, reforçam a necessidade de bases integradas e coerentes para que algoritmos de aprendizagem de máquina captem padrões multivariados reprodutíveis \cite{nascimento2011analise,andrianarivony2024review}.

Um pipeline explícito de \textit{Extract–Transform–Load} sustenta reprodutibilidade, rastreabilidade e controle de qualidade. Arquiteturas modulares e parametrizadas reduzem a dívida técnica de fluxos ad hoc, estabelecem contratos claros de entrada/saída e preservam a proveniência das transformações, favorecendo manutenção e reexecução em novos períodos e recortes espaciais \cite{sculley2014hidden}. A integração entre focos e clima demanda harmonização de formatos e unidades, resolução de chaves espaço-temporais e normalização de códigos sentinela, produzindo um conjunto tabular padronizado, adequado à comparação justa entre modelos.

Séries meteorológicas apresentam padrões sistemáticos de ausência por falhas de sensores, interrupções e mudanças operacionais; ignorar lacunas distorce estatísticas, compromete variáveis derivadas e afeta previsores treinados sob regularidade temporal \cite{alejosanchez2025review}. Imputação constitui um modelo adicional aplicado aos dados de entrada e pode alterar distribuições e extremos; a literatura em clima de alta resolução mostra que a escolha do método deve considerar mecanismo de ausência e estrutura temporal/espacial \cite{afridayamoah2020imputation}. Não há técnica universalmente superior: desempenho depende de variável, resolução e padrão de faltantes \cite{alejosanchez2025review}.

Como diretriz prática e transparente, adota-se KNN para imputação numérica quando a taxa de faltantes e as correlações locais o justificam, mantendo cenários paralelos sem imputação para avaliar sensibilidade das conclusões. Estudos comparativos indicam que KNN apresenta desempenho competitivo sob ausências leves e aleatórias, com baixo custo e interpretabilidade, sendo adequado como baseline de imputação \cite{chehal2023imputation}. Em variáveis climaticamente centrais e frequentemente incompletas (por exemplo, radiação), a estratégia combina: (i) bases sem imputação ou com exclusão seletiva e (ii) bases com KNN, permitindo quantificar o impacto da imputação na comparação entre modelos.

\section{Comparação de modelos} \label{sec:comparacao-modelos}

A opção por comparação sistemática entre modelos decorre de um princípio teórico e de evidência empírica. Teoricamente, os resultados No Free Lunch demonstram que, sem suposições fortes sobre a distribuição geradora, não há preferência a priori entre algoritmos de aprendizado supervisionado; para quaisquer dois métodos, existem tantos alvos em que um supera o outro quanto o inverso \cite{wolpert1996lack}. Em problemas reais, escolhas devem ser sustentadas por avaliação empírica controlada, com protocolos que reduzam viés de partição e permitam estimativas estáveis de desempenho.

Estudos comparativos recentes em dados tabulares mostram que ensembles de árvores por \emph{boosting} tendem a liderar em acurácia, precisão, F1 e ROC-AUC, enquanto abordagens baseadas em margem (SVM) destacam-se em \emph{recall} e modelos lineares (regressão logística) apresentam custo computacional inferior, preservando interpretabilidade \cite{martinovic2025comparative}. Essa heterogeneidade de pontos fortes por métrica justifica incluir famílias distintas (lineares, margem, gerativos, árvores e \emph{boosting}) e reportar múltiplos indicadores de desempenho.

O protocolo de comparação deve alinhar-se às características do problema: desbalanceamento da classe positiva e necessidade de escores probabilísticos calibráveis. Métricas centradas na detecção de raros, como PR-AUC, \emph{recall} e F1, são prioritárias, complementadas por Brier score e curvas de calibração para avaliar a qualidade probabilística. A validação deve empregar partições espaço-temporais e repetições com testes estatísticos corrigidos para mitigar vitórias espúrias por sobreposição de dobras \cite{martinovic2025comparative}. Quando houver cenários alternativos de tratamento de dados (com e sem imputação), a comparação deve ser replicada em cada cenário, pois a imputação altera a distribuição das entradas e pode mudar o ranking relativo dos modelos.


