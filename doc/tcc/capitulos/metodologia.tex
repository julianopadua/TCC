% !TEX root = ./main.tex

\chapter{Metodologia}
\label{cap:metodologia}

Neste capítulo são descritos o fluxo de dados, os procedimentos de preparação das bases e a forma como os conjuntos finais foram utilizados na etapa de modelagem. A ênfase está no pipeline de \textit{Extract-Transform-Load} (ETL) desenvolvido especificamente para este trabalho, que integra focos de calor do BDQueimadas e variáveis climáticas do INMET, com foco no bioma Cerrado.

\section{Obtenção e organização dos dados}
\label{sec:obtencao-dados}

Os dados de focos de calor foram obtidos a partir do sistema BDQueimadas (INPE), na coleção anual Brasil\_sat\_ref, enquanto as variáveis climáticas foram extraídas das séries de estações automáticas do INMET. A obtenção não foi feita de forma manual pontual, mas estruturada em um pipeline de scripts Python, localizado na pasta \texttt{src/} do repositório.

De maneira resumida, são utilizadas duas frentes complementares do BDQueimadas:

\begin{itemize}
  \item \textbf{Exportações manuais} (\texttt{data/raw/BDQUEIMADAS/}): arquivos \texttt{exportador\_*{\_}ref{\_}YYYY.csv} baixados diretamente pelo portal, que contêm variáveis de risco e física do foco (como \texttt{RiscoFogo} e \texttt{FRP}), mas não incluem o identificador interno \texttt{foco\_id};
  \item \textbf{Arquivos oficiais Brasil\_sat\_ref} (\texttt{data/raw/ID\_BDQUEIMADAS/}): arquivos \texttt{focos\_br\_ref\_YYYY.zip}, baixados automaticamente do \textit{dataserver} do COIDS pelo script \texttt{bdqueimadas\_scraper.py}, que, após extração, fornecem \texttt{foco\_id}, \texttt{id\_bdq} e coordenadas geográficas.
\end{itemize}

Os dados meteorológicos são obtidos pelo módulo \texttt{inmet\_scraper.py}, que coleta e organiza as séries do INMET em \texttt{data/processed/INMET/}, padronizando nomes de colunas e unidades para que diferentes anos e estações possam ser combinados com os focos de calor.

A Figura~\ref{fig:pipeline-dataset} apresenta, em alto nível, o fluxo de dados construído neste trabalho, desde a ingestão das fontes brutas até a geração das bases anuais em formato \texttt{.parquet} utilizadas na modelagem.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{imagens/dataset_pipeline.png}
    \caption{Fluxo geral do pipeline de dados BDQueimadas + INMET.}
    \label{fig:pipeline-dataset}
\end{figure}

Os arquivos de dados são organizados na pasta \texttt{data/} de forma hierárquica:

\begin{itemize}
    \item \texttt{data/raw/}: dados brutos (downloads automáticos e exportações manuais);
    \item \texttt{data/processed/}: saídas intermediárias após consolidação interna de cada fonte;
    \item \texttt{data/external/}: consolidações que integram diferentes camadas do BDQueimadas (manual e processado);
    \item \texttt{data/dataset/}: arquivos anuais \texttt{inmet\_bdq\_ANO\_cerrado.csv}, já integrando focos e variáveis climáticas;
    \item \texttt{data/eda/}: resultados das auditorias de dados faltantes;
    \item \texttt{data/modeling/}: bases finais de modelagem para cada cenário de tratamento de \textit{missing}.
\end{itemize}

Essa estrutura, configurada via \texttt{config.yaml} e acessada pelos utilitários de \texttt{utils.py}, permite reproduzir todo o fluxo apenas executando os scripts na ordem descrita nas próximas seções.

\section{Pipeline de extração e consolidação}
\label{sec:pipeline-extracao}

O pipeline de ETL foi construído em blocos modulares, cada um responsável por uma etapa bem definida: ingestão do BDQueimadas, ingestão do INMET, integração das duas fontes e filtragem para o bioma Cerrado.

\subsection{Consolidação do BDQueimadas}

A consolidação dos focos de calor é feita pelo módulo \texttt{consolidated\_bdqueimadas.py}. O objetivo é combinar, para cada ano, as exportações manuais e os arquivos oficiais Brasil\_sat\_ref em uma única base consolidada.

Para isso, o script lê:

\begin{itemize}
    \item o arquivo manual \texttt{exportador\_*{\_}ref{\_}YYYY.csv} em \texttt{data/raw/BDQUEIMADAS/};
    \item o arquivo \texttt{focos\_br\_ref\_YYYY.csv} extraído em \texttt{data/processed/ID\_BDQUEIMADAS/focos\_br\_ref\_YYYY/}.
\end{itemize}

A junção é feita a partir de uma chave sintética \texttt{\_\_KEY}, composta por data e hora arredondadas à hora cheia, país, unidade da federação e município, após normalização de acentos e conversão para uma forma ASCII maiúscula. No lado manual, a data-hora é extraída da coluna \texttt{DataHora}; no lado processado, da coluna \texttt{data\_pas}.

Com essa chave é realizado um \textit{left join} das exportações manuais com o conjunto processado, de modo que cada foco do arquivo manual receba, quando possível, os identificadores e coordenadas do arquivo oficial. O resultado agrega:

\begin{itemize}
    \item do manual: \texttt{RiscoFogo}, \texttt{FRP} e demais variáveis descritivas;
    \item do processado: \texttt{FOCO\_ID}, \texttt{ID\_BDQ}, latitude e longitude.
\end{itemize}

Os produtos finais são arquivos \texttt{bdq\_targets\_*.csv}, gravados em \texttt{data/external/BDQUEIMADAS/}, que podem representar um único ano ou intervalos multi-anos, com filtragem opcional por bioma (por exemplo, Cerrado).

\subsection{Consolidação do INMET e integração BDQueimadas + INMET}

Os módulos \texttt{inmet\_scraper.py} e \texttt{inmet\_consolidated.py} são responsáveis por baixar e padronizar as séries do INMET. Conceitualmente, o fluxo segue os passos:

\begin{enumerate}
    \item seleção das estações automáticas relevantes para o bioma Cerrado e para o período de interesse;
    \item download das séries horárias, utilizando a API ou interface web do INMET;
    \item padronização de nomes de colunas, unidades e formatos de data/hora;
    \item gravação de arquivos já consolidados em \texttt{data/processed/INMET/}.
\end{enumerate}

A integração entre focos e variáveis climáticas é feita pelo módulo \texttt{build\_dataset.py}, que consome simultaneamente as consolidações do BDQueimadas e do INMET. Esse script:

\begin{itemize}
    \item filtra os registros para o bioma Cerrado, utilizando a classificação presente nos próprios arquivos de focos;
    \item alinha temporalmente os dados de estação e os focos, garantindo consistência na granularidade utilizada;
    \item constrói, para cada ano, um arquivo \texttt{inmet\_bdq\_ANO\_cerrado.csv} em \texttt{data/dataset/}.
\end{itemize}

Nessa base integrada é criada a variável binária \texttt{HAS\_FOCO}, que indica se, naquele registro (combinação de local e instante de tempo), houve ao menos um foco de queimada registrado no BDQueimadas. Essa variável é a base para a tarefa de classificação tratada na etapa de modelagem.

\section{Tratamento de dados faltantes e bases de modelagem}
\label{sec:bases-modelagem}

\subsection{Auditoria de dados faltantes e códigos sentinela}

Antes da construção das bases de modelagem, foi necessário definir de forma clara o que seria considerado dado faltante. Para isso foi desenvolvido o módulo \texttt{dataset\_missing\_audit.py}, que aplica uma semântica unificada de \textit{missing} às colunas das bases integradas \texttt{inmet\_bdq\_ANO\_cerrado.csv}.

São tratados como valores faltantes:

\begin{itemize}
    \item valores \texttt{NaN} ou nulos;
    \item códigos sentinela numéricos \texttt{-999} e \texttt{-9999};
    \item as strings \texttt{"-999"} e \texttt{"-9999"} em colunas textuais;
    \item strings vazias após aplicação de \textit{strip}.
\end{itemize}

Para cada ano, o script gera uma matriz booleana de faltantes e produz um resumo em \texttt{data/eda/dataset/ANO/}, contendo:

\begin{itemize}
    \item um arquivo \texttt{missing\_by\_column.csv} com contagem e proporção de faltantes por coluna, separando registros com e sem foco;
    \item um \texttt{README\_missing.md} que descreve o número total de linhas, a proporção de linhas com foco e as colunas mais críticas em termos de ausência de dados.
\end{itemize}

Algumas colunas são explicitamente excluídas da análise de \textit{features}, como identificadores temporais e espaciais (\texttt{DATA (YYYY-MM-DD)}, \texttt{HORA (UTC)}, \texttt{CIDADE}, \texttt{cidade\_norm}, \texttt{ts\_hour}, \texttt{ANO}), a variável \texttt{HAS\_FOCO} e as colunas diretamente associadas ao foco (\texttt{RISCO\_FOGO}, \texttt{FRP}, \texttt{FOCO\_ID}).

\subsection{Construção das bases de modelagem}

A partir dos arquivos \texttt{inmet\_bdq\_ANO\_cerrado.csv}, o módulo \texttt{modeling\_build\_datasets.py} gera múltiplos cenários de base, já em formato \texttt{.parquet}, armazenados em \texttt{data/modeling/}. Antes de separar os cenários, são aplicadas duas padronizações:

\begin{enumerate}
    \item \textbf{Harmonização da radiação global}: quando a variável de radiação aparece com nomes ligeiramente diferentes em anos distintos (por exemplo, \texttt{RADIACAO GLOBAL (Kj/m²)} e \texttt{RADIACAO GLOBAL (KJ/m²)}), os valores são unificados em uma única coluna canônica, descartando duplicatas;
    \item \textbf{Aplicação da semântica de \textit{missing}}: os códigos sentinela numéricos são convertidos em \texttt{NaN} nas colunas numéricas, e strings vazias são convertidas em \texttt{NaN} nas colunas textuais.
\end{enumerate}

Em seguida, são definidas as colunas de \textit{features} como todas aquelas que não pertencem aos conjuntos:

\begin{itemize}
    \item colunas alvo: \texttt{RISCO\_FOGO}, \texttt{FRP}, \texttt{FOCO\_ID};
    \item coluna de rótulo: \texttt{HAS\_FOCO};
    \item colunas de identificação temporal e espacial, como \texttt{ANO}, \texttt{DATA (YYYY-MM-DD)}, \texttt{HORA (UTC)}, \texttt{CIDADE}, \texttt{cidade\_norm}, \texttt{ts\_hour}.
\end{itemize}

Essas \textit{features} são convertidas para tipo numérico (\texttt{errors="coerce"}), de modo que valores inconsistentes passem a ser tratados como \textit{missing} e entrem na lógica já descrita.

Com essa base harmonizada, o script gera, para cada ano, seis cenários distintos, variando a presença da radiação global e a estratégia de tratamento de dados faltantes:

\begin{itemize}
    \item \textbf{base\_F\_full\_original}: mantém a radiação; aplica apenas harmonização e conversão de sentinelas para \textit{missing}, sem remoção de linhas nem imputação;
    \item \textbf{base\_A\_no\_rad}: remove a coluna de radiação (se existir), mantendo as demais variáveis e sem remoção de linhas;
    \item \textbf{base\_B\_no\_rad\_knn}: parte de \texttt{base\_A} e aplica \textit{KNNImputer} nas colunas numéricas de \textit{features};
    \item \textbf{base\_C\_no\_rad\_drop\_rows}: parte de \texttt{base\_A} e remove todas as linhas com qualquer \textit{missing} em \textit{features};
    \item \textbf{base\_D\_with\_rad\_drop\_rows}: parte da base com radiação e remove todas as linhas com \textit{missing} em \textit{features};
    \item \textbf{base\_E\_with\_rad\_knn}: parte da base com radiação e aplica \textit{KNNImputer} nas \textit{features} numéricas, incluindo a radiação.
\end{itemize}

Esses cenários permitem comparar, na etapa de modelagem, o impacto da presença da variável de radiação e de diferentes estratégias de tratamento de dados faltantes sobre o desempenho dos modelos preditivos.

Ao final, toda a implementação do pipeline de ETL, scripts de auditoria e construção das bases de modelagem encontra-se disponível em repositório público de código\footnote{Substituir pelo endereço oficial do repositório, por exemplo: \url{https://github.com/seu-usuario/seu-repo-tcc}}.
